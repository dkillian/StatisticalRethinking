---
title: 'Statistical Rethinking'
subtitle: "Chapter 6: Overfitting, Regularization, and Information Criteria"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '5'
    toc_float: yes
---

### Prep

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=10, fig.height=6, warning=FALSE, message=FALSE, cache=T)
```

```{r include=F}

library(tidyverse)
library(broom)
library(psych)
library(haven)
library(sjmisc)
library(sjPlot)
library(rstanarm)
library(brms)
library(lme4)
library(rethinking)

setwd("C:/Users/Dan/Dropbox/personal/Coursera/R stuff/Statistical Rethinking")

```


```{r include=F}
options(digits=3, scipen=6)

# vertical and horizontal lines
theme_set(theme_bw() + theme(panel.grid.minor.x=element_blank(),
                             panel.grid.minor.y=element_blank(),
                             plot.title=element_text(face="bold",size=16, hjust=.5),
                             axis.title=element_text(size=14),
                             axis.text=element_text(size=12),
                             strip.text=element_text(size=12)))
```

### 6.1 The problem with parameters

Overfitting occurs when a model learns too much from the sample. A sample contains both regular and irregular features. We are interested in the regular features of the data, where 'regular' could be denoted as the characteristics of the underlying data generation process that we are interested in. Irregular features are those that are more unique to the data sample and do not generalize to the data generation process. We want the sample to tell us about the regular features of the data, not the irregular features. 

Here is an example of overfitting: 

```{r}

## R code 6.1
sppnames <- c( "afarensis","africanus","habilis","boisei",
               "rudolfensis","ergaster","sapiens")
brainvolcc <- c( 438 , 452 , 612, 521, 752, 871, 1350 )
masskg <- c( 37.0 , 35.5 , 34.5 , 41.5 , 55.5 , 61.0 , 53.5 )
d <- data.frame( species=sppnames , brain=brainvolcc , mass=masskg )
d

```

```{r}
describe(d)
```

```{r}

ggplot(d, aes(x=mass, y=brain, label=species)) + 
  geom_text(nudge_y=2, nudge_x=.2) + 
  stat_smooth(method="lm", se=F) + 
  stat_smooth(se=F, color="darkgoldenrod") +
  stat_smooth(se=F, formula= y~poly(x,2), color="maroon") + 
  stat_smooth(se=F, formula= y ~ poly(x,3), color="purple")
  ylim(c(200,1400))

```

The idea is that the second and third order polynomial fits may capture more of the variation in the sample, but will perform terribly on new samples of the data. A simple linear approximation may be the regular feature, but this is lost due to overfitting. 

```{r}

## R code 6.2
m6.1 <- lm( brain ~ mass , data=d )
precis(m6.1)
```

```{r}
summary(m6.1)
```
```{r}
# manual R squared
1 - var(resid(m6.1)) / var(d$brain)
```
```{r}

## R code 6.4
m6.2 <- lm( brain ~ mass + I(mass^2) , data=d )
summary(m6.2)

```

Notice how the R squared measure increases. 

```{r}

## R code 6.5
m6.3 <- lm( brain ~ mass + I(mass^2) + I(mass^3) , data=d )
m6.4 <- lm( brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) ,
            data=d )
m6.5 <- lm( brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) +
                 I(mass^5) , data=d )
m6.6 <- lm( brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) +
                 I(mass^5) + I(mass^6) , data=d )

sapply(list(m6.3, m6.4, m6.5, m6.6), function(x) 1-var(resid(x)) / var(d$brain))
```

Plot an example. 

```{r}
m6.4a <- augment(m6.4)
ggplot(m6.4a, aes(x=mass, y=.fitted)) + 
  geom_point() + 
  stat_smooth(se=F)
```

If we add enough parameters, we could make the line go through every point. 

A model can also underfit. Consider a predictive model that is just the mean of the sample. 

```{r}

## R code 6.6
m6.7 <- lm( brain ~ 1 , data=d )

ggplot(d, aes(x=mass, y=brain)) + 
  geom_point() + 
  geom_hline(yintercept=mean(d$brain))



```

One way to think about overfitting and underfitting is how sensitive the model is to the sample. An underfit model will be insensitive to the sample - removing a datum will not change the predicted value by much. Removing a datum from a model with several polynomials will change the path of the predicted values quite a lot. 

Here's leave-one-out predictions from the underfit model: 

```{r}

## R code 6.8
plot( brain ~ mass , d , col="slateblue" ) + 
for ( i in 1:nrow(d) ) {
     d.new <- d[ -i , ]
     m0 <- lm( brain ~ mass, d.new )
     abline( m0 , col=col.alpha("black",0.5) )
}

```

Ideas of the bias / variance tradeoff are related to under- and overfitting, respectively. 

### 6.2 Information theory and model performance

Information theory provides a target for a model to aspire to: deviance. 

Deviance is an approximation of relative distance from perfect accuracy. This despite the fact that we don't know what perfrect accuracy is - we don't the the absolute truth of what we're investigating; we don't have a 'true' model that mimics the data generating process in nature. 

But we can somehow use the relative deviance of competing models to help us discriminate between models. Let's see if we can figure this out. 

#### 6.2.1 Predicting the weather

A weatherperson makes a 10 forecast, and a newcomer says he can best that forecast by predicting sun every day. 

```{r}
day <- 1:10
prediction1 <- c(1,1,1,.6,.6,.6,.6,.6,.6,.6)
prediction2 <- rep(0,10)
outcome <- c(rep(1,3), rep(0,7))
pred <- data.frame(day, prediction1, prediction2, outcome)
pred
```

Define hit rate as the average probability of a correct prediction. 

```{r}
hit1 <- (3*1) + (7 * .4)
hit2 <- (3*0) + (7*1)
out <- c(hit1, hit2)
out
```

The newcomer who predicted sunshine every day wins. 

But now let's assign costs to being wrong. 

Let's say carrying an umbrella is scored -1 and getting caught in the rain without an umbrella -5. Your chance of being caught without an umbrella is the forecast probability of rain. 

```{r}
points1 <- c(-1,-1,-1,-.6, -.6, -.6, -.6, -.6, -.6, -.6)
points2 <- c(-5,-5,-5, rep(0,7))
points <- c(sum(points1), sum(points2))
points
```

Now the weatherperson wins. 

Going back to a measure of accuracy ignoring costs and benefits, consider the probability of getting the 10-day forecast exactly right. This would be the joint probability of all days, which is the same as the joint likelihood in Bayes Theorem. 

```{r}
jnt1 <- 1^3 * .4^7
jnt2 <- 0^3 * 1^7
jnt <- c(jnt1, jnt2)
jnt # it's .005 in text
```

The weatherperson wins again. 

#### 6.2.2 Information and uncertainty

So we've identified joint probability as our target accuracy measure. How to measure distance from this target? 

Define information as the reduction in uncertainty upon learning an outcome. 

Uncertainty about an outcome can then be defined in terms of information entropy. 

$$ H(p) = \sum_{i=1}^{n}p_{i}log(p_{i}) $$

The uncertainty contained in a probability distribution is the average log-probability of an event. 

Suppose we have a true probability of rain of .3 and true probability of sun of .7. 

The information entropy of the weather forecast is then (p_rain * log(p_rain)) + (p_sun * log(p_sun))

```{r}
-( (.3 * log(.3)) + (.7 * log(.7)) )
```


```{r}
## R code 6.9
p <- c( 0.3 , 0.7 )
-sum( p*log(p) )

```

Now consider a weather forecast in Abu Dhabi of .01 for rain and .99 for sun. 

``` {r}
p <- c(.01, .99)
-sum(p*log(p))
```

Uncertainty has decreased to .06 because it so seldom rains. 

Adding more events increases uncertainty. Consider adding snow to the forecast. 

```{r}
p <- c(.7, .15, .15)
-sum(p*log(p))
```

Information entropy is a measure of how hard it is to hit the target. Now we want to use this accuracy measure to derive a measure of how far a model is from the target. 

#### 6.2.3 From entropy to accuracy

Divergence (cross-entropy) is the additional uncertainty induced by using probabilities from one distribution to describe another distribution. 

This is Kullback-Leibler divergence, or K-L divergence. Our familiar measure of R-squared turns out to be a special case of K-L divergence.  

K-L divergence is simply the difference in entropies of the target p and the model q. 

```{r}

# target p
p <- c(.3,.7)
p_ent <- -sum(p*log(p))
p_ent
```

```{r}

# model q
q <- c(.25, .75)
q_ent <- -sum(q*log(q))
q_ent

```

```{r}
p_ent - q_ent
```

More formally: 

$$ D_{KL}(p,q) = \sum_{i}p_{i}(log(p_{i})) - log(q_{i}) $$

$$ D_{KL}(p,q) = \sum_{i}p_{i}log\left(\frac{p_{i}}{q_{i}}\right) $$
Divergence is the average difference in log probability between the target p and model q. 

```{r}
.3*log(.3 / .25)
```


```{r}
.7*log(.7/.75)
```

```{r}
q <- tibble(p1 = .3,
            p2 = .7,
            q1 = seq(.01,.99,.01),
            q2 = 1-q1,
            dkl = p1 * log(p1/q1) + p2 * log(p2/q2))
q
```

```{r}
library(directlabels)

ggplot(q, aes(x=q1, y=dkl)) + 
  stat_smooth(se=F) + 
  geom_vline(xintercept=.3, color="maroon", size=1) + 
  scale_x_continuous(breaks=seq(0,1,.1)) +
  scale_y_continuous(limits=c(0,2.5)) 
```

That recreates the text figure. Let's do our own with both q1 and q2. 

```{r}
q_2 <- q %>%
  gather(prob, val, -p1, -p2)
head(q_2)

```

```{r}

ggplot(q, aes(y=dkl)) + 
  stat_smooth(aes(x=q1), color="maroon", se=F) + 
  geom_vline(xintercept=.3, color="maroon", size=1) +
  stat_smooth(aes(x=q2), color="darkblue", se=F) + 
  geom_vline(xintercept=.7, color="darkblue", size=1) +
  scale_x_continuous(breaks=seq(0,1,.1)) +
  scale_y_continuous(limits=c(0,2.5)) +
  labs(x="prob",
       y="divergance",
       title="") 
```

Overthinking: divergence depends on direction



#### 6.2.4 From divergence to deviance

All of the discussion so far is based on comparing a prediction from a known target. What if we don't know the target, but have only a set of predictions? Turns out you can still use entropy to compare models against one another, even when the 'true' model remains unknown. 

Define a model's deviance as negative twice the log likelihood of each observation in the model. 

$$ -2\sum_{i}log(q_{i}) $$

```{r}
sppnames <- c( "afarensis","africanus","habilis","boisei",
               "rudolfensis","ergaster","sapiens")

brainvolcc <- c( 438 , 452 , 612, 521, 752, 871, 1350 )
masskg <- c( 37.0 , 35.5 , 34.5 , 41.5 , 55.5 , 61.0 , 53.5 )

d <- data.frame( species=sppnames , brain=brainvolcc , mass=masskg )
d

```

```{r}
m6.1 <- lm(brain ~ mass, d)

-2*logLik(m6.1)
```
94.9

Now calculate directly

```{r}

d$mass.s <- (d$mass - mean(d$mass)) / sd(d$mass)

m6.8 <- map(
  alist(
    brain ~ dnorm(mu, sigma),
    mu <- a + b*mass.s
  ),
data=d,
start=list(a=mean(d$brain), b=0, sigma=sd(d$brain)),
method="Nelder-Mead"
)

precis(m6.8)

```

```{r}
theta <- coef(m6.8)
theta
```

Book code:

```{r}

dev <- (-2)*sum(dnorm(d$brain,
                    mean=theta[1] + theta[2]*d$mass.s,
                    sd=theta[3],
                    log=T))
dev
```

It replicates but let's unpack this. 

```{r}
?dnorm

d <- d %>%
  mutate(pred=theta[1] + theta[2]*mass.s)

d

```

So we have actual and predicted brain sizes, and we compute the log likelihood of each brain size relative to the model of all brain sizes. 

```{r}
d <- d %>%
  mutate(lik = dnorm(brain, mean=pred, sd=theta[3], log=T))

d$lik <- dnorm(d$brain, mean=theta[1] + theta[2]*d$mass.s,
                sd=theta[3],
                log=T)
d
```

Recall that dnorm gives us a measure of the probability of observing a given value, given some parameter value. The lik variable is the log of this probability for each observation of the model (relative to the parameters of the model), and the log liklihood of the entire model is then the sum of these * -2.  

```{r}
sum(d$lik) * -2
```



#### 6.2.5 From deviance to out-of-sample


```{r}

```












